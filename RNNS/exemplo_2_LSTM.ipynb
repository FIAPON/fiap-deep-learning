{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"5b2497b3-60ee-7cd0-0625-f103214c0ed4","id":"Pc7TriuxTCZu"},"source":["# **Análise de Sentimento:**\n","\n"," o processo de identificação e categorização computacional das opiniões expressas em um pedaço de texto, especialmente com o objetivo de determinar se a atitude do escritor em relação a um tópico específico, produto, etc., é positiva, negativa ou neutra.\n","\n"]},{"cell_type":"code","source":["import os\n","os.environ['KAGGLE_CONFIG_DIR'] = \"/content\"\n","!chmod 600 /content/kaggle.json\n","\n","\n","!kaggle datasets download -d crowdflower/first-gop-debate-twitter-sentiment\n","!unzip /content/first-gop-debate-twitter-sentiment -d /content/kaggle/"],"metadata":{"id":"LdT5ORNSTFB0"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6c53202d-5c34-4859-e7e9-8ef5c7068287","id":"J1CbO2UATCZx"},"outputs":[],"source":["\n","\n","import numpy as np\n","import pandas as pd\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Dense, Embedding, LSTM\n","from sklearn.model_selection import train_test_split\n","from keras.utils import to_categorical\n","import re\n"]},{"cell_type":"markdown","metadata":{"_cell_guid":"2bc2702e-d6f4-df5f-b80e-50ab23a6d29e","id":"whM0_oLlTCZx"},"source":["Matendo apenas a colunas necessárias"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"89c8c923-c0bf-7b35-9ab8-e63f00b74e5a","id":"IMmfKHZnTCZx"},"outputs":[],"source":["data = pd.read_csv('/content/kaggle/Sentiment.csv')\n","data = data[['text','sentiment']]"]},{"cell_type":"markdown","metadata":{"_cell_guid":"4c0ec63b-cdf8-8e29-812b-0fbbfcea2929","id":"V4B3V_JSTCZy"},"source":["A seguir, estou eliminando os sentimentos 'Neutros', pois meu objetivo era apenas diferenciar tweets positivos e negativos. Depois disso, estou filtrando os tweets para que apenas textos e palavras válidas permaneçam. Em seguida, defino o número máximo de recursos como 2000 e uso o Tokenizer para vetorizar e converter o texto em Sequências para que a Rede possa lidar com ele como entrada."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"43632d2d-6160-12ce-48b0-e5eb1c207076","id":"4co34BA0TCZy"},"outputs":[],"source":["data = data[data.sentiment != \"Neutral\"]\n","data['text'] = data['text'].apply(lambda x: x.lower())\n","data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n","\n","print(data[ data['sentiment'] == 'Positive'].size)\n","print(data[ data['sentiment'] == 'Negative'].size)\n","\n","for idx,row in data.iterrows():\n","    row[0] = row[0].replace('rt',' ')\n","\n","max_fatures = 2000\n","tokenizer = Tokenizer(nb_words=max_fatures, split=' ')\n","tokenizer.fit_on_texts(data['text'].values)\n","X = tokenizer.texts_to_sequences(data['text'].values)\n","X = pad_sequences(X)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"9753421e-1303-77d5-b17f-5f25fa08c452","id":"joj_2qc-TCZz"},"source":["A seguir, eu configuro a Rede LSTM. Observe que as variáveis **embed_dim**, **lstm_out**, **batch_size** e **dropout_x** são hiperparâmetros; seus valores são de certa forma intuitivos e podem e devem ser ajustados para obter bons resultados. Por favor, observe também que estou usando a função de ativação softmax. A razão para isso é que nossa Rede está utilizando a entropia cruzada categórica (categorical crossentropy), e softmax é o método de ativação adequado para isso.\n","\n"," Cria uma rede neural para tarefas de classificação de texto usando uma camada de incorporação seguida por uma camada LSTM e uma camada de saída softmax. O modelo pode ser treinado com dados de texto rotulados para realizar tarefas de classificação, como análise de sentimento."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1ba3cf60-a83c-9c21-05e0-b14303027e93","id":"IZ7F8YPvTCZ0"},"outputs":[],"source":["from keras.models import Sequential\n","from keras.layers import Embedding, LSTM, Dense, Dropout\n","\n","embed_dim = 128  # Numero de dimensões para a camada de incorporação\n","lstm_out = 196  # número de unidades (neurônios) na camada LSTM\n","\n","model = Sequential()\n","model.add(Embedding(max_fatures, embed_dim, input_length=X.shape[1]))\n","model.add(Dropout(0.2))\n","model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n","model.add(Dense(2, activation='softmax'))\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(model.summary())\n"]},{"cell_type":"markdown","metadata":{"_cell_guid":"15f4ee61-47e4-88c4-4b81-98a85237333f","id":"l8_CmJCpTCZ1"},"source":["Dataset para treino e teste"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b35748b8-2353-3db2-e571-5fd22bb93eb0","id":"Hp3jhZswTCZ1"},"outputs":[],"source":["Y = pd.get_dummies(data['sentiment']).values\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.33, random_state = 42)\n","print(X_train.shape,Y_train.shape)\n","print(X_test.shape,Y_test.shape)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"2a775979-a930-e627-2963-18557d7bf6e6","id":"-UVMif_zTCZ2"},"source":["Treinamento"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d5e499ac-2eba-6ff7-8d9a-ff65eb04099b","id":"aiwP921yTCZ2"},"outputs":[],"source":["batch_size = 32\n","model.fit(X_train, Y_train, epochs = 25, batch_size=batch_size, verbose = 1)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"4ebd7bc1-53c0-0e31-a0b0-b6d0a3017434","id":"lKM8DqAjTCZ2"},"source":["Validação do treinamento"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a970f412-722f-6d6d-72c8-325d0901ccef","id":"YYO4nPsDTCZ2"},"outputs":[],"source":["validation_size = 1500\n","\n","X_validate = X_test[-validation_size:]\n","Y_validate = Y_test[-validation_size:]\n","X_test = X_test[:-validation_size]\n","Y_test = Y_test[:-validation_size]\n","score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n","print(\"score: %.2f\" % (score))\n","print(\"acc: %.2f\" % (acc))"]},{"cell_type":"markdown","metadata":{"_cell_guid":"018ebf39-9414-27d0-232c-a34de051feaf","id":"sDBDxP1jTCZ2"},"source":["Finalmente, estamos medindo o número de suposições corretas. Está claro que a Rede se sai muito bem ao identificar tweets negativos, mas não é tão eficaz em decidir se um tweet é positivo. Minha suposição educada aqui é que o conjunto de treinamento de tweets positivos é dramaticamente menor do que o de tweets negativos, o que explica os resultados \"ruins\" para os tweets positivos."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1add73e9-c6fb-7e4c-8715-ea92f519d2a6","id":"gePk37H8TCZ3"},"outputs":[],"source":["pos_cnt, neg_cnt, pos_correct, neg_correct = 0, 0, 0, 0\n","for x in range(len(X_validate)):\n","\n","    result = model.predict(X_validate[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n","\n","    if np.argmax(result) == np.argmax(Y_validate[x]):\n","        if np.argmax(Y_validate[x]) == 0:\n","            neg_correct += 1\n","        else:\n","            pos_correct += 1\n","\n","    if np.argmax(Y_validate[x]) == 0:\n","        neg_cnt += 1\n","    else:\n","        pos_cnt += 1\n","\n","\n","\n","print(\"pos_acc\", pos_correct/pos_cnt*100, \"%\")\n","print(\"neg_acc\", neg_correct/neg_cnt*100, \"%\")"]},{"cell_type":"code","source":["# Save the model to a file\n","model.save(\"sentiment_model_2.h5\", save_format='tf')\n"],"metadata":{"id":"easZNFKgf-G1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.models import load_model\n","\n","# Load do modelo\n","loaded_model = load_model(\"sentiment_model.h5\")\n","\n","# Agora você pode usar o load_model para fazer previsões sobre novos dados\n","# Por exemplo, você pode usar o tokenizer para pré-processar novos dados de texto e depois usar carregado_model.predict()\n","\n","new_text = [\" Have a excellent day\", \"This is a good review\", \"I didn't like this movie\", 'Meetings: Because none of us is as dumb as all of us.', \"I love Domino's pizza\"]\n","new_text = [text.lower() for text in new_text]\n","new_text = [re.sub('[^a-zA-z0-9\\s]', '', text) for text in new_text]\n","\n","# Tokenize and pad the new text data\n","new_sequences = tokenizer.texts_to_sequences(new_text)\n","new_sequences = pad_sequences(new_sequences, maxlen=X.shape[1])\n","\n","# Make predictions\n","predictions = loaded_model.predict(new_sequences)\n","\n","# You can interpret the predictions based on your label encoding\n","for prediction in predictions:\n","    if np.argmax(prediction) == 0:\n","        print(\"Negative\")\n","    else:\n","        print(\"Positive\")\n"],"metadata":{"id":"iiu15Qywf_ZB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"yQp3hWXFwL5U"},"execution_count":null,"outputs":[]}],"metadata":{"_change_revision":185,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}