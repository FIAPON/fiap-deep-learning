{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kz9Nq4kW6Rpy"
      },
      "source": [
        "## Geração Automática de Texto com LSTMs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYNETVuP6Rp0"
      },
      "source": [
        "As Redes Neurais Recorrentes também funcionam como modelos generativos. Além de prever, elas aprendem sequências de um problema e podem criar novas sequências plausíveis no mesmo domínio. Esses modelos generativos são valiosos não só para avaliar o aprendizado de um problema, mas também para entender melhor o domínio em questão.\n",
        "\n",
        "Para aprimorar a habilidade de escrita, é útil usar livros clássicos, onde já temos familiaridade com a história e não violamos direitos autorais. Muitos livros clássicos estão em domínio público e podem ser acessados gratuitamente. O Projeto Gutenberg é uma ótima fonte para encontrar esses livros. Utilizaremos \"Alice no País das Maravilhas\" ou \"Alice's Adventures in Wonderland\" em inglês. O arquivo txt do livro está disponível em https://www.gutenberg.org/ebooks/11 ou anexado a este Jupyter Notebook, contendo cerca de 3.300 linhas de texto, sem o cabeçalho e a marca de final de arquivo.\n",
        "\n",
        "Exploraremos as relações entre caracteres e suas probabilidades condicionais em sequências para gerar novas sequências de caracteres originais. É uma atividade divertida e sugiro experimentar com outros livros do Projeto Gutenberg. Além de texto, é possível usar outros dados ASCII, como código de programação, documentos em LaTeX, HTML ou Markdown, entre outros.\n",
        "\n",
        "Nossa abordagem será similar à do programador destacada neste artigo: http://www.businessinsider.com/ai-just-wrote-the-next-book-of-game-of-thrones-for-us-2017-8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qtSa3frq6Rp0",
        "outputId": "3ae02bf1-7720-4325-aad7-0c2a290fe7c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'fiap-deep-learning'...\n",
            "remote: Enumerating objects: 8267, done.\u001b[K\n",
            "remote: Counting objects: 100% (104/104), done.\u001b[K\n",
            "remote: Compressing objects: 100% (87/87), done.\u001b[K\n",
            "remote: Total 8267 (delta 48), reused 46 (delta 15), pack-reused 8163\u001b[K\n",
            "Receiving objects: 100% (8267/8267), 533.65 MiB | 22.89 MiB/s, done.\n",
            "Resolving deltas: 100% (51/51), done.\n",
            "Updating files: 100% (8238/8238), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/FIAPON/fiap-deep-learning.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7IHxFsx56Rp1",
        "outputId": "cd1784de-8703-4cf8-b3df-1271b8c92995",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/fiap-deep-learning/RNNS\n"
          ]
        }
      ],
      "source": [
        "%cd /content/fiap-deep-learning/RNNS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "E0WVmh466Rp1"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import numpy\n",
        "import sys\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "id": "_7-yGqoB6Rp2"
      },
      "outputs": [],
      "source": [
        "# Carregamos os dados e convertemos para lowercase\n",
        "filename = \"dataset/wonderland.txt\"\n",
        "raw_text = open(filename).read()\n",
        "raw_text = raw_text.lower()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGzcQMNr6Rp2"
      },
      "source": [
        "Agora que o livro está carregado, devemos preparar os dados para modelagem. Não podemos modelar os caracteres diretamente, em vez disso, devemos converter os caracteres em números inteiros. Podemos fazer isso facilmente, criando um conjunto de todos os caracteres distintos do livro, então criando um mapa de cada caractere para um único inteiro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "id": "mgiENyrt6Rp2"
      },
      "outputs": [],
      "source": [
        "# Criando o mapeamento caracter/inteiro\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6hMShYwM6Rp3",
        "outputId": "b5decfa5-dbcf-4a8d-fe3d-fad07e6a2990",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n',\n",
              " ' ',\n",
              " '!',\n",
              " '\"',\n",
              " \"'\",\n",
              " '(',\n",
              " ')',\n",
              " '*',\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " ':',\n",
              " ';',\n",
              " '?',\n",
              " '[',\n",
              " ']',\n",
              " '_',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z',\n",
              " '\\ufeff']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZX3rhkbE6Rp3",
        "outputId": "f5af168c-9111-4c75-9d0f-6dbff09c1e94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\n': 0,\n",
              " ' ': 1,\n",
              " '!': 2,\n",
              " '\"': 3,\n",
              " \"'\": 4,\n",
              " '(': 5,\n",
              " ')': 6,\n",
              " '*': 7,\n",
              " ',': 8,\n",
              " '-': 9,\n",
              " '.': 10,\n",
              " ':': 11,\n",
              " ';': 12,\n",
              " '?': 13,\n",
              " '[': 14,\n",
              " ']': 15,\n",
              " '_': 16,\n",
              " 'a': 17,\n",
              " 'b': 18,\n",
              " 'c': 19,\n",
              " 'd': 20,\n",
              " 'e': 21,\n",
              " 'f': 22,\n",
              " 'g': 23,\n",
              " 'h': 24,\n",
              " 'i': 25,\n",
              " 'j': 26,\n",
              " 'k': 27,\n",
              " 'l': 28,\n",
              " 'm': 29,\n",
              " 'n': 30,\n",
              " 'o': 31,\n",
              " 'p': 32,\n",
              " 'q': 33,\n",
              " 'r': 34,\n",
              " 's': 35,\n",
              " 't': 36,\n",
              " 'u': 37,\n",
              " 'v': 38,\n",
              " 'w': 39,\n",
              " 'x': 40,\n",
              " 'y': 41,\n",
              " 'z': 42,\n",
              " '\\ufeff': 43}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "char_to_int"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t09V62n_6Rp3"
      },
      "source": [
        "Pode haver alguns caracteres que podemos remover para limpar mais o conjunto de dados que reduzirá o vocabulário e poderá melhorar o processo de modelagem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "j1dPWTJz6Rp4",
        "outputId": "78a6ccad-ba68-422c-bce8-6b44b5333a56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Characters:  144343\n",
            "Total Vocab:  44\n"
          ]
        }
      ],
      "source": [
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print (\"Total Characters: \", n_chars)\n",
        "print (\"Total Vocab: \", n_vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12-5_73f6Rp4"
      },
      "source": [
        "Podemos ver que o livro tem pouco menos de 150.000 caracteres e que quando convertidos para minúsculas, existem apenas 44 caracteres distintos no vocabulário para a rede aprender, muito mais do que os 26 no alfabeto. Agora, precisamos definir os dados de treinamento para a rede. Existe muita flexibilidade em como você escolhe dividir o texto e expô-lo a rede durante o treino. Aqui dividiremos o texto do livro em subsequências com um comprimento de 100 caracteres, um comprimento arbitrário. Poderíamos facilmente dividir os dados por sentenças e ajustar as sequências mais curtas e truncar as mais longas. Cada padrão de treinamento da rede é composto de 100 passos de tempo (time steps) de um caractere (X) seguido por um caracter de saída (y). Ao criar essas sequências, deslizamos esta janela ao longo de todo o livro um caracter de cada vez, permitindo que cada caracter tenha a chance de ser aprendido a partir dos 100 caracteres que o precederam (exceto os primeiros 100 caracteres, é claro). Por exemplo, se o comprimento da sequência é 5 (para simplificar), os dois primeiros padrões de treinamento seriam os seguintes:\n",
        "\n",
        "* Palavra: CHAPTER\n",
        "* CHAPT -> E\n",
        "* HAPTE -> R"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0QeFuGCN6Rp4",
        "outputId": "da523e72-743c-4d85-e3d1-ca03a8d175ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de Padrões:  144243\n"
          ]
        }
      ],
      "source": [
        "# À medida que dividimos o livro em sequências, convertemos os caracteres em números inteiros usando nossa\n",
        "# tabela de pesquisa que preparamos anteriormente.\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "    seq_in = raw_text[i:i + seq_length]\n",
        "    seq_out = raw_text[i + seq_length]\n",
        "    dataX.append([char_to_int[char] for char in seq_in])\n",
        "    dataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print (\"Total de Padrões: \", n_patterns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQreOiMZ6Rp5"
      },
      "source": [
        "Agora que preparamos nossos dados de treinamento, precisamos transformá-lo para que possamos usá-lo com o Keras. Primeiro, devemos transformar a lista de sequências de entrada na forma [amostras, passos de tempo, recursos] esperados por uma rede LSTM. Em seguida, precisamos redimensionar os números inteiros para o intervalo de 0 a 1 para tornar os padrões mais fáceis de aprender pela rede LSTM que usa a função de ativação sigmoide por padrão."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": true,
        "id": "ZwDMTZpY6Rp5"
      },
      "outputs": [],
      "source": [
        "# Reshape de X para [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "\n",
        "# Normalização\n",
        "X = X / float(n_vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2mU4t6r6Rp5"
      },
      "source": [
        "Finalmente, precisamos converter os padrões de saída (caracteres únicos convertidos em números inteiros) usando Hot-Encoding. Isto é para que possamos configurar a rede para prever a probabilidade de cada um dos 44 caracteres diferentes no vocabulário (uma representação mais fácil) em vez de tentar forçá-lo a prever com precisão o próximo caracter. Cada valor de y é convertido em um vetor com um comprimento 44, cheio de zeros, exceto com um 1 na coluna para a letra (inteiro) que o padrão representa. Por exemplo, quando a letra n (valor inteiro 30) tiver sido transformada usando One-Hot Encoding, vai se parecer com isso:\n",
        "\n",
        "[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": true,
        "id": "ESvAzONp6Rp6"
      },
      "outputs": [],
      "source": [
        "# One-Hot Encoding da variável de saída\n",
        "y = to_categorical(dataY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": true,
        "id": "v-bVqlO46Rp6"
      },
      "outputs": [],
      "source": [
        "# Modelo LSTM com duas camadas de Dropout com 20%\n",
        "# O tempo de treinamento é bem longo\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfI8IfWN6Rp6"
      },
      "source": [
        "Não há conjunto de dados de teste. Estamos modelando todo o conjunto de dados de treinamento para aprender a probabilidade de cada caracter em uma sequência. Não estamos interessados nos mais preciso modelo do conjunto de dados de treinamento (Acurácia de Classificação). Este seria um modelo que prevê cada caracter no conjunto de dados de treinamento perfeitamente. Em vez disso, estamos interessados em uma generalização do conjunto de dados que minimiza a função de perda escolhida. Estamos buscando um equilíbrio entre generalização e\n",
        "overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": true,
        "id": "eTSwPeSn6Rp6"
      },
      "outputs": [],
      "source": [
        "# Define o checkpoint\n",
        "filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor = 'loss', verbose = 1, save_best_only = True, mode = 'min')\n",
        "callbacks_list = [checkpoint]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jm2pa_6d6Rp6"
      },
      "source": [
        "Fit do modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ex5Yv0HB6Rp6",
        "outputId": "32c10523-401f-4980-b1bd-ca758ff30dd8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "2254/2254 [==============================] - ETA: 0s - loss: 2.7736\n",
            "Epoch 1: loss improved from inf to 2.77360, saving model to weights-improvement-01-2.7736.hdf5\n",
            "2254/2254 [==============================] - 46s 18ms/step - loss: 2.7736\n",
            "Epoch 2/50\n",
            "   7/2254 [..............................] - ETA: 39s - loss: 2.5264"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2253/2254 [============================>.] - ETA: 0s - loss: 2.3935\n",
            "Epoch 2: loss improved from 2.77360 to 2.39349, saving model to weights-improvement-02-2.3935.hdf5\n",
            "2254/2254 [==============================] - 42s 19ms/step - loss: 2.3935\n",
            "Epoch 3/50\n",
            "2254/2254 [==============================] - ETA: 0s - loss: 2.1911\n",
            "Epoch 3: loss improved from 2.39349 to 2.19110, saving model to weights-improvement-03-2.1911.hdf5\n",
            "2254/2254 [==============================] - 42s 19ms/step - loss: 2.1911\n",
            "Epoch 4/50\n",
            "2252/2254 [============================>.] - ETA: 0s - loss: 2.0591\n",
            "Epoch 4: loss improved from 2.19110 to 2.05900, saving model to weights-improvement-04-2.0590.hdf5\n",
            "2254/2254 [==============================] - 43s 19ms/step - loss: 2.0590\n",
            "Epoch 5/50\n",
            "2253/2254 [============================>.] - ETA: 0s - loss: 1.9624\n",
            "Epoch 5: loss improved from 2.05900 to 1.96239, saving model to weights-improvement-05-1.9624.hdf5\n",
            "2254/2254 [==============================] - 42s 19ms/step - loss: 1.9624\n",
            "Epoch 6/50\n",
            "2253/2254 [============================>.] - ETA: 0s - loss: 1.8909\n",
            "Epoch 6: loss improved from 1.96239 to 1.89084, saving model to weights-improvement-06-1.8908.hdf5\n",
            "2254/2254 [==============================] - 42s 19ms/step - loss: 1.8908\n",
            "Epoch 7/50\n",
            "2253/2254 [============================>.] - ETA: 0s - loss: 1.8278\n",
            "Epoch 7: loss improved from 1.89084 to 1.82778, saving model to weights-improvement-07-1.8278.hdf5\n",
            "2254/2254 [==============================] - 42s 19ms/step - loss: 1.8278\n",
            "Epoch 8/50\n",
            "2252/2254 [============================>.] - ETA: 0s - loss: 1.7748\n",
            "Epoch 8: loss improved from 1.82778 to 1.77486, saving model to weights-improvement-08-1.7749.hdf5\n",
            "2254/2254 [==============================] - 43s 19ms/step - loss: 1.7749\n",
            "Epoch 9/50\n",
            "2254/2254 [==============================] - ETA: 0s - loss: 1.7290\n",
            "Epoch 9: loss improved from 1.77486 to 1.72900, saving model to weights-improvement-09-1.7290.hdf5\n",
            "2254/2254 [==============================] - 42s 19ms/step - loss: 1.7290\n",
            "Epoch 10/50\n",
            "2254/2254 [==============================] - ETA: 0s - loss: 1.6864\n",
            "Epoch 10: loss improved from 1.72900 to 1.68641, saving model to weights-improvement-10-1.6864.hdf5\n",
            "2254/2254 [==============================] - 42s 19ms/step - loss: 1.6864\n",
            "Epoch 11/50\n",
            "2254/2254 [==============================] - ETA: 0s - loss: 1.6520\n",
            "Epoch 11: loss improved from 1.68641 to 1.65205, saving model to weights-improvement-11-1.6520.hdf5\n",
            "2254/2254 [==============================] - 42s 19ms/step - loss: 1.6520\n",
            "Epoch 12/50\n",
            "2252/2254 [============================>.] - ETA: 0s - loss: 1.6204\n",
            "Epoch 12: loss improved from 1.65205 to 1.62044, saving model to weights-improvement-12-1.6204.hdf5\n",
            "2254/2254 [==============================] - 42s 19ms/step - loss: 1.6204\n",
            "Epoch 13/50\n",
            "2253/2254 [============================>.] - ETA: 0s - loss: 1.5905\n",
            "Epoch 13: loss improved from 1.62044 to 1.59057, saving model to weights-improvement-13-1.5906.hdf5\n",
            "2254/2254 [==============================] - 42s 19ms/step - loss: 1.5906\n",
            "Epoch 14/50\n",
            "2253/2254 [============================>.] - ETA: 0s - loss: 1.5601\n",
            "Epoch 14: loss improved from 1.59057 to 1.56022, saving model to weights-improvement-14-1.5602.hdf5\n",
            "2254/2254 [==============================] - 42s 19ms/step - loss: 1.5602\n",
            "Epoch 15/50\n",
            "2254/2254 [==============================] - ETA: 0s - loss: 1.5339\n",
            "Epoch 15: loss improved from 1.56022 to 1.53393, saving model to weights-improvement-15-1.5339.hdf5\n",
            "2254/2254 [==============================] - 42s 19ms/step - loss: 1.5339\n",
            "Epoch 16/50\n",
            "2254/2254 [==============================] - ETA: 0s - loss: 1.5095\n",
            "Epoch 16: loss improved from 1.53393 to 1.50945, saving model to weights-improvement-16-1.5095.hdf5\n",
            "2254/2254 [==============================] - 42s 18ms/step - loss: 1.5095\n",
            "Epoch 17/50\n",
            "2252/2254 [============================>.] - ETA: 0s - loss: 1.4886\n",
            "Epoch 17: loss improved from 1.50945 to 1.48853, saving model to weights-improvement-17-1.4885.hdf5\n",
            "2254/2254 [==============================] - 42s 19ms/step - loss: 1.4885\n",
            "Epoch 18/50\n",
            "2254/2254 [==============================] - ETA: 0s - loss: 1.4627\n",
            "Epoch 18: loss improved from 1.48853 to 1.46275, saving model to weights-improvement-18-1.4627.hdf5\n",
            "2254/2254 [==============================] - 42s 19ms/step - loss: 1.4627\n",
            "Epoch 19/50\n",
            "2254/2254 [==============================] - ETA: 0s - loss: 1.4448\n",
            "Epoch 19: loss improved from 1.46275 to 1.44480, saving model to weights-improvement-19-1.4448.hdf5\n",
            "2254/2254 [==============================] - 42s 18ms/step - loss: 1.4448\n",
            "Epoch 20/50\n",
            "2253/2254 [============================>.] - ETA: 0s - loss: 1.4297\n",
            "Epoch 20: loss improved from 1.44480 to 1.42973, saving model to weights-improvement-20-1.4297.hdf5\n",
            "2254/2254 [==============================] - 42s 19ms/step - loss: 1.4297\n",
            "Epoch 21/50\n",
            "2254/2254 [==============================] - ETA: 0s - loss: 1.4086\n",
            "Epoch 21: loss improved from 1.42973 to 1.40862, saving model to weights-improvement-21-1.4086.hdf5\n",
            "2254/2254 [==============================] - 42s 18ms/step - loss: 1.4086\n",
            "Epoch 22/50\n",
            "2254/2254 [==============================] - ETA: 0s - loss: 1.3960\n",
            "Epoch 22: loss improved from 1.40862 to 1.39599, saving model to weights-improvement-22-1.3960.hdf5\n",
            "2254/2254 [==============================] - 42s 19ms/step - loss: 1.3960\n",
            "Epoch 23/50\n",
            "2254/2254 [==============================] - ETA: 0s - loss: 1.3806\n",
            "Epoch 23: loss improved from 1.39599 to 1.38063, saving model to weights-improvement-23-1.3806.hdf5\n",
            "2254/2254 [==============================] - 42s 19ms/step - loss: 1.3806\n",
            "Epoch 24/50\n",
            "2253/2254 [============================>.] - ETA: 0s - loss: 1.3613\n",
            "Epoch 24: loss improved from 1.38063 to 1.36126, saving model to weights-improvement-24-1.3613.hdf5\n",
            "2254/2254 [==============================] - 42s 19ms/step - loss: 1.3613\n",
            "Epoch 25/50\n",
            "2254/2254 [==============================] - ETA: 0s - loss: 1.3541\n",
            "Epoch 25: loss improved from 1.36126 to 1.35412, saving model to weights-improvement-25-1.3541.hdf5\n",
            "2254/2254 [==============================] - 42s 19ms/step - loss: 1.3541\n",
            "Epoch 26/50\n",
            "2253/2254 [============================>.] - ETA: 0s - loss: 1.3406\n",
            "Epoch 26: loss improved from 1.35412 to 1.34052, saving model to weights-improvement-26-1.3405.hdf5\n",
            "2254/2254 [==============================] - 42s 19ms/step - loss: 1.3405\n",
            "Epoch 27/50\n",
            "2253/2254 [============================>.] - ETA: 0s - loss: 1.3288\n",
            "Epoch 27: loss improved from 1.34052 to 1.32887, saving model to weights-improvement-27-1.3289.hdf5\n",
            "2254/2254 [==============================] - 42s 19ms/step - loss: 1.3289\n",
            "Epoch 28/50\n",
            "2254/2254 [==============================] - ETA: 0s - loss: 1.3156\n",
            "Epoch 28: loss improved from 1.32887 to 1.31558, saving model to weights-improvement-28-1.3156.hdf5\n",
            "2254/2254 [==============================] - 42s 18ms/step - loss: 1.3156\n",
            "Epoch 29/50\n",
            "2254/2254 [==============================] - ETA: 0s - loss: 1.3084\n",
            "Epoch 29: loss improved from 1.31558 to 1.30842, saving model to weights-improvement-29-1.3084.hdf5\n",
            "2254/2254 [==============================] - 42s 19ms/step - loss: 1.3084\n",
            "Epoch 30/50\n",
            "2253/2254 [============================>.] - ETA: 0s - loss: 1.2977\n",
            "Epoch 30: loss improved from 1.30842 to 1.29763, saving model to weights-improvement-30-1.2976.hdf5\n",
            "2254/2254 [==============================] - 42s 19ms/step - loss: 1.2976\n",
            "Epoch 31/50\n",
            "2254/2254 [==============================] - ETA: 0s - loss: 1.2899\n",
            "Epoch 31: loss improved from 1.29763 to 1.28985, saving model to weights-improvement-31-1.2899.hdf5\n",
            "2254/2254 [==============================] - 42s 19ms/step - loss: 1.2899\n",
            "Epoch 32/50\n",
            "2253/2254 [============================>.] - ETA: 0s - loss: 1.2818\n",
            "Epoch 32: loss improved from 1.28985 to 1.28181, saving model to weights-improvement-32-1.2818.hdf5\n",
            "2254/2254 [==============================] - 42s 19ms/step - loss: 1.2818\n",
            "Epoch 33/50\n",
            "2253/2254 [============================>.] - ETA: 0s - loss: 1.2780\n",
            "Epoch 33: loss improved from 1.28181 to 1.27802, saving model to weights-improvement-33-1.2780.hdf5\n",
            "2254/2254 [==============================] - 42s 19ms/step - loss: 1.2780\n",
            "Epoch 34/50\n",
            "2253/2254 [============================>.] - ETA: 0s - loss: 1.2656\n",
            "Epoch 34: loss improved from 1.27802 to 1.26565, saving model to weights-improvement-34-1.2657.hdf5\n",
            "2254/2254 [==============================] - 42s 19ms/step - loss: 1.2657\n",
            "Epoch 35/50\n",
            "2254/2254 [==============================] - ETA: 0s - loss: 1.2609\n",
            "Epoch 35: loss improved from 1.26565 to 1.26092, saving model to weights-improvement-35-1.2609.hdf5\n",
            "2254/2254 [==============================] - 42s 18ms/step - loss: 1.2609\n",
            "Epoch 36/50\n",
            "2254/2254 [==============================] - ETA: 0s - loss: 1.2554\n",
            "Epoch 36: loss improved from 1.26092 to 1.25539, saving model to weights-improvement-36-1.2554.hdf5\n",
            "2254/2254 [==============================] - 42s 19ms/step - loss: 1.2554\n",
            "Epoch 37/50\n",
            "2252/2254 [============================>.] - ETA: 0s - loss: 1.2483\n",
            "Epoch 37: loss improved from 1.25539 to 1.24850, saving model to weights-improvement-37-1.2485.hdf5\n",
            "2254/2254 [==============================] - 42s 19ms/step - loss: 1.2485\n",
            "Epoch 38/50\n",
            "2253/2254 [============================>.] - ETA: 0s - loss: 1.2446\n",
            "Epoch 38: loss improved from 1.24850 to 1.24455, saving model to weights-improvement-38-1.2445.hdf5\n",
            "2254/2254 [==============================] - 42s 19ms/step - loss: 1.2445\n",
            "Epoch 39/50\n",
            "2254/2254 [==============================] - ETA: 0s - loss: 1.2398\n",
            "Epoch 39: loss improved from 1.24455 to 1.23983, saving model to weights-improvement-39-1.2398.hdf5\n",
            "2254/2254 [==============================] - 42s 18ms/step - loss: 1.2398\n",
            "Epoch 40/50\n",
            "2254/2254 [==============================] - ETA: 0s - loss: 1.2326\n",
            "Epoch 40: loss improved from 1.23983 to 1.23258, saving model to weights-improvement-40-1.2326.hdf5\n",
            "2254/2254 [==============================] - 42s 19ms/step - loss: 1.2326\n",
            "Epoch 41/50\n",
            "2254/2254 [==============================] - ETA: 0s - loss: 1.2272\n",
            "Epoch 41: loss improved from 1.23258 to 1.22722, saving model to weights-improvement-41-1.2272.hdf5\n",
            "2254/2254 [==============================] - 42s 19ms/step - loss: 1.2272\n",
            "Epoch 42/50\n",
            "2253/2254 [============================>.] - ETA: 0s - loss: 1.2185\n",
            "Epoch 42: loss improved from 1.22722 to 1.21846, saving model to weights-improvement-42-1.2185.hdf5\n",
            "2254/2254 [==============================] - 42s 19ms/step - loss: 1.2185\n",
            "Epoch 43/50\n",
            "2254/2254 [==============================] - ETA: 0s - loss: 1.2211\n",
            "Epoch 43: loss did not improve from 1.21846\n",
            "2254/2254 [==============================] - 42s 19ms/step - loss: 1.2211\n",
            "Epoch 44/50\n",
            "2252/2254 [============================>.] - ETA: 0s - loss: 1.2145\n",
            "Epoch 44: loss improved from 1.21846 to 1.21461, saving model to weights-improvement-44-1.2146.hdf5\n",
            "2254/2254 [==============================] - 42s 19ms/step - loss: 1.2146\n",
            "Epoch 45/50\n",
            "2252/2254 [============================>.] - ETA: 0s - loss: 1.2121\n",
            "Epoch 45: loss improved from 1.21461 to 1.21220, saving model to weights-improvement-45-1.2122.hdf5\n",
            "2254/2254 [==============================] - 42s 19ms/step - loss: 1.2122\n",
            "Epoch 46/50\n",
            "2254/2254 [==============================] - ETA: 0s - loss: 1.2053\n",
            "Epoch 46: loss improved from 1.21220 to 1.20528, saving model to weights-improvement-46-1.2053.hdf5\n",
            "2254/2254 [==============================] - 42s 19ms/step - loss: 1.2053\n",
            "Epoch 47/50\n",
            "2253/2254 [============================>.] - ETA: 0s - loss: 1.2064\n",
            "Epoch 47: loss did not improve from 1.20528\n",
            "2254/2254 [==============================] - 42s 19ms/step - loss: 1.2065\n",
            "Epoch 48/50\n",
            "2254/2254 [==============================] - ETA: 0s - loss: 1.1998\n",
            "Epoch 48: loss improved from 1.20528 to 1.19976, saving model to weights-improvement-48-1.1998.hdf5\n",
            "2254/2254 [==============================] - 43s 19ms/step - loss: 1.1998\n",
            "Epoch 49/50\n",
            "2253/2254 [============================>.] - ETA: 0s - loss: 1.1974\n",
            "Epoch 49: loss improved from 1.19976 to 1.19740, saving model to weights-improvement-49-1.1974.hdf5\n",
            "2254/2254 [==============================] - 42s 19ms/step - loss: 1.1974\n",
            "Epoch 50/50\n",
            "2252/2254 [============================>.] - ETA: 0s - loss: 1.1945\n",
            "Epoch 50: loss improved from 1.19740 to 1.19444, saving model to weights-improvement-50-1.1944.hdf5\n",
            "2254/2254 [==============================] - 42s 19ms/step - loss: 1.1944\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e1d19b11960>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# model.fit(X, y, epochs = 20, batch_size = 128, callbacks = callbacks_list)\n",
        "model.fit(X, y, epochs = 50, batch_size = 64, callbacks = callbacks_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeW-s99f6Rp7"
      },
      "source": [
        "Depois de executar o fit, você deve ter uma série de arquivos de checkpoint no mesmo diretório onde está este Jupyter Notebook. Você pode excluí-los todos exceto aquele com o menor valor de perda. Por exemplo, neste caso, o arquivo weights-improvement-19-1.9119.hdf5 será usado. Ele contém os melhores valores de peso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "collapsed": true,
        "id": "YN8iRvXY6Rp7"
      },
      "outputs": [],
      "source": [
        "# Carrega os melhores pesos da rede e compila o modelo\n",
        "filename = \"/content/fiap-deep-learning/RNNS/weights-improvement-50-1.1944.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "collapsed": true,
        "id": "ynKOfgZl6Rp7"
      },
      "outputs": [],
      "source": [
        "int_to_char = dict((i, c) for i, c in enumerate(chars))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNLyOSLd6Rp7",
        "outputId": "275ae94b-4b51-4884-86d4-b2abf30b9f2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\" d it out, we should all have our heads cut off, you know.\n",
            "so you see, miss, we're doing our best, af \"\n",
            "iore the tame words ' \n",
            "'i'm a pittle shing i don't know what the rame thing!' said the dormouse, who was see anything to see it tailing to the wood, who was seeding to herself, 'i dare was the tood, and was in one fres to say '\n",
            "\n",
            "'i con't know what this you?' said the dormouse, 'or i'd nade of the sood of the words ' (and the mock turtle said this she was suill in a mong wa"
          ]
        }
      ],
      "source": [
        "# Obtém um random seed\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "\n",
        "# Inicia a geração de texto de um ponto qualquer, definido pelo random seed \"start\"\n",
        "pattern = dataX[start]\n",
        "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "\n",
        "# Gerando caracteres\n",
        "for i in range(1000):\n",
        "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "    x = x / float(n_vocab)\n",
        "    prediction = model.predict(x, verbose=0)\n",
        "    index = numpy.argmax(prediction)\n",
        "    result = int_to_char[index]\n",
        "    seq_in = [int_to_char[value] for value in pattern]\n",
        "    sys.stdout.write(result)\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]\n",
        "print (\"\\nConcluído.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "qBbF_S-b6Rp7"
      },
      "source": [
        "Abaixo algumas sugestões para melhorar este modelo:\n",
        "\n",
        "* Prever menos de 1.000 caracteres como saída para uma determinada semente (seed).\n",
        "* Remova toda a pontuação do texto original e, portanto, do vocabulário do modelo.\n",
        "* Experimente One-Hot Encoding para as sequências de entrada.\n",
        "* Aumente o número de épocas de treinamento para 100 ou muitas centenas (isso pode levar até dias para o treinamento, mas aumentará a previsão do modelo.\n",
        "* Ajuste o percentual de Dropout\n",
        "* Adicione mais unidades de memória às camadas e/ou mais camadas."
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}