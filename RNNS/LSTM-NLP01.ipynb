{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geração Automática de Texto com LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Redes Neurais Recorrentes também funcionam como modelos generativos. Além de prever, elas aprendem sequências de um problema e podem criar novas sequências plausíveis no mesmo domínio. Esses modelos generativos são valiosos não só para avaliar o aprendizado de um problema, mas também para entender melhor o domínio em questão.\n",
    "\n",
    "Para aprimorar a habilidade de escrita, é útil usar livros clássicos, onde já temos familiaridade com a história e não violamos direitos autorais. Muitos livros clássicos estão em domínio público e podem ser acessados gratuitamente. O Projeto Gutenberg é uma ótima fonte para encontrar esses livros. Utilizaremos \"Alice no País das Maravilhas\" ou \"Alice's Adventures in Wonderland\" em inglês. O arquivo txt do livro está disponível em https://www.gutenberg.org/ebooks/11 ou anexado a este Jupyter Notebook, contendo cerca de 3.300 linhas de texto, sem o cabeçalho e a marca de final de arquivo.\n",
    "\n",
    "Exploraremos as relações entre caracteres e suas probabilidades condicionais em sequências para gerar novas sequências de caracteres originais. É uma atividade divertida e sugiro experimentar com outros livros do Projeto Gutenberg. Além de texto, é possível usar outros dados ASCII, como código de programação, documentos em LaTeX, HTML ou Markdown, entre outros.\n",
    "\n",
    "Nossa abordagem será similar à do programador destacada neste artigo: http://www.businessinsider.com/ai-just-wrote-the-next-book-of-game-of-thrones-for-us-2017-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/FIAPON/fiap-deep-learning.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/fiap-deep-learning/RNNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy\n",
    "import sys\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Carregamos os dados e convertemos para lowercase \n",
    "filename = \"dataset/wonderland.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que o livro está carregado, devemos preparar os dados para modelagem. Não podemos modelar os caracteres diretamente, em vez disso, devemos converter os caracteres em números inteiros. Podemos fazer isso facilmente, criando um conjunto de todos os caracteres distintos do livro, então criando um mapa de cada caractere para um único inteiro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Criando o mapeamento caracter/inteiro\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '\"',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " '[',\n",
       " ']',\n",
       " '_',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '\\ufeff']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '!': 2,\n",
       " '\"': 3,\n",
       " \"'\": 4,\n",
       " '(': 5,\n",
       " ')': 6,\n",
       " '*': 7,\n",
       " ',': 8,\n",
       " '-': 9,\n",
       " '.': 10,\n",
       " ':': 11,\n",
       " ';': 12,\n",
       " '?': 13,\n",
       " '[': 14,\n",
       " ']': 15,\n",
       " '_': 16,\n",
       " 'a': 17,\n",
       " 'b': 18,\n",
       " 'c': 19,\n",
       " 'd': 20,\n",
       " 'e': 21,\n",
       " 'f': 22,\n",
       " 'g': 23,\n",
       " 'h': 24,\n",
       " 'i': 25,\n",
       " 'j': 26,\n",
       " 'k': 27,\n",
       " 'l': 28,\n",
       " 'm': 29,\n",
       " 'n': 30,\n",
       " 'o': 31,\n",
       " 'p': 32,\n",
       " 'q': 33,\n",
       " 'r': 34,\n",
       " 's': 35,\n",
       " 't': 36,\n",
       " 'u': 37,\n",
       " 'v': 38,\n",
       " 'w': 39,\n",
       " 'x': 40,\n",
       " 'y': 41,\n",
       " 'z': 42,\n",
       " '\\ufeff': 43}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pode haver alguns caracteres que podemos remover para limpar mais o conjunto de dados que reduzirá o vocabulário e poderá melhorar o processo de modelagem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  144343\n",
      "Total Vocab:  44\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que o livro tem pouco menos de 150.000 caracteres e que quando convertidos para minúsculas, existem apenas 44 caracteres distintos no vocabulário para a rede aprender, muito mais do que os 26 no alfabeto. Agora, precisamos definir os dados de treinamento para a rede. Existe muita flexibilidade em como você escolhe dividir o texto e expô-lo a rede durante o treino. Aqui dividiremos o texto do livro em subsequências com um comprimento de 100 caracteres, um comprimento arbitrário. Poderíamos facilmente dividir os dados por sentenças e ajustar as sequências mais curtas e truncar as mais longas. Cada padrão de treinamento da rede é composto de 100 passos de tempo (time steps) de um caractere (X) seguido por um caracter de saída (y). Ao criar essas sequências, deslizamos esta janela ao longo de todo o livro um caracter de cada vez, permitindo que cada caracter tenha a chance de ser aprendido a partir dos 100 caracteres que o precederam (exceto os primeiros 100 caracteres, é claro). Por exemplo, se o comprimento da sequência é 5 (para simplificar), os dois primeiros padrões de treinamento seriam os seguintes:\n",
    "\n",
    "* Palavra: CHAPTER\n",
    "* CHAPT -> E\n",
    "* HAPTE -> R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de Padrões:  144243\n"
     ]
    }
   ],
   "source": [
    "# À medida que dividimos o livro em sequências, convertemos os caracteres em números inteiros usando nossa\n",
    "# tabela de pesquisa que preparamos anteriormente.\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total de Padrões: \", n_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que preparamos nossos dados de treinamento, precisamos transformá-lo para que possamos usá-lo com o Keras. Primeiro, devemos transformar a lista de sequências de entrada na forma [amostras, passos de tempo, recursos] esperados por uma rede LSTM. Em seguida, precisamos redimensionar os números inteiros para o intervalo de 0 a 1 para tornar os padrões mais fáceis de aprender pela rede LSTM que usa a função de ativação sigmoide por padrão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reshape de X para [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "\n",
    "# Normalização\n",
    "X = X / float(n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, precisamos converter os padrões de saída (caracteres únicos convertidos em números inteiros) usando Hot-Encoding. Isto é para que possamos configurar a rede para prever a probabilidade de cada um dos 44 caracteres diferentes no vocabulário (uma representação mais fácil) em vez de tentar forçá-lo a prever com precisão o próximo caracter. Cada valor de y é convertido em um vetor com um comprimento 44, cheio de zeros, exceto com um 1 na coluna para a letra (inteiro) que o padrão representa. Por exemplo, quando a letra n (valor inteiro 30) tiver sido transformada usando One-Hot Encoding, vai se parecer com isso:\n",
    "\n",
    "[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One-Hot Encoding da variável de saída\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Modelo LSTM com duas camadas de Dropout com 20%\n",
    "# O tempo de treinamento é bem longo\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não há conjunto de dados de teste. Estamos modelando todo o conjunto de dados de treinamento para aprender a probabilidade de cada caracter em uma sequência. Não estamos interessados nos mais preciso modelo do conjunto de dados de treinamento (Acurácia de Classificação). Este seria um modelo que prevê cada caracter no conjunto de dados de treinamento perfeitamente. Em vez disso, estamos interessados em uma generalização do conjunto de dados que minimiza a função de perda escolhida. Estamos buscando um equilíbrio entre generalização e\n",
    "overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define o checkpoint\n",
    "filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor = 'loss', verbose = 1, save_best_only = True, mode = 'min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 2.8677Epoch 00000: loss improved from inf to 2.86788, saving model to weights-improvement-00-2.8679.hdf5\n",
      "144243/144243 [==============================] - 534s - loss: 2.8679   \n",
      "Epoch 2/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 3.0639Epoch 00001: loss did not improve\n",
      "144243/144243 [==============================] - 533s - loss: 3.0639   \n",
      "Epoch 3/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 3.0446Epoch 00002: loss did not improve\n",
      "144243/144243 [==============================] - 528s - loss: 3.0445   \n",
      "Epoch 4/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 2.6626Epoch 00003: loss improved from 2.86788 to 2.66252, saving model to weights-improvement-03-2.6625.hdf5\n",
      "144243/144243 [==============================] - 526s - loss: 2.6625   \n",
      "Epoch 5/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 2.4720Epoch 00004: loss improved from 2.66252 to 2.47194, saving model to weights-improvement-04-2.4719.hdf5\n",
      "144243/144243 [==============================] - 526s - loss: 2.4719   \n",
      "Epoch 6/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 2.3304Epoch 00005: loss improved from 2.47194 to 2.33029, saving model to weights-improvement-05-2.3303.hdf5\n",
      "144243/144243 [==============================] - 528s - loss: 2.3303   \n",
      "Epoch 7/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 2.2254Epoch 00006: loss improved from 2.33029 to 2.22532, saving model to weights-improvement-06-2.2253.hdf5\n",
      "144243/144243 [==============================] - 534s - loss: 2.2253   \n",
      "Epoch 8/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 2.1438Epoch 00007: loss improved from 2.22532 to 2.14387, saving model to weights-improvement-07-2.1439.hdf5\n",
      "144243/144243 [==============================] - 532s - loss: 2.1439   \n",
      "Epoch 9/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 2.0786Epoch 00008: loss improved from 2.14387 to 2.07849, saving model to weights-improvement-08-2.0785.hdf5\n",
      "144243/144243 [==============================] - 536s - loss: 2.0785   \n",
      "Epoch 10/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 2.0192Epoch 00009: loss improved from 2.07849 to 2.01909, saving model to weights-improvement-09-2.0191.hdf5\n",
      "144243/144243 [==============================] - 532s - loss: 2.0191   \n",
      "Epoch 11/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.9703Epoch 00010: loss improved from 2.01909 to 1.97029, saving model to weights-improvement-10-1.9703.hdf5\n",
      "144243/144243 [==============================] - 536s - loss: 1.9703   \n",
      "Epoch 12/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.9253Epoch 00011: loss improved from 1.97029 to 1.92522, saving model to weights-improvement-11-1.9252.hdf5\n",
      "144243/144243 [==============================] - 529s - loss: 1.9252   \n",
      "Epoch 13/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.8829Epoch 00012: loss improved from 1.92522 to 1.88296, saving model to weights-improvement-12-1.8830.hdf5\n",
      "144243/144243 [==============================] - 529s - loss: 1.8830   \n",
      "Epoch 14/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.8515Epoch 00013: loss improved from 1.88296 to 1.85149, saving model to weights-improvement-13-1.8515.hdf5\n",
      "144243/144243 [==============================] - 534s - loss: 1.8515   \n",
      "Epoch 15/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.8149Epoch 00014: loss improved from 1.85149 to 1.81484, saving model to weights-improvement-14-1.8148.hdf5\n",
      "144243/144243 [==============================] - 530s - loss: 1.8148   \n",
      "Epoch 16/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.7850Epoch 00015: loss improved from 1.81484 to 1.78503, saving model to weights-improvement-15-1.7850.hdf5\n",
      "144243/144243 [==============================] - 533s - loss: 1.7850   \n",
      "Epoch 17/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.7620Epoch 00016: loss improved from 1.78503 to 1.76201, saving model to weights-improvement-16-1.7620.hdf5\n",
      "144243/144243 [==============================] - 524s - loss: 1.7620   \n",
      "Epoch 18/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.7303Epoch 00017: loss improved from 1.76201 to 1.73034, saving model to weights-improvement-17-1.7303.hdf5\n",
      "144243/144243 [==============================] - 528s - loss: 1.7303   \n",
      "Epoch 19/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.7047Epoch 00018: loss improved from 1.73034 to 1.70467, saving model to weights-improvement-18-1.7047.hdf5\n",
      "144243/144243 [==============================] - 532s - loss: 1.7047   \n",
      "Epoch 20/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.6832Epoch 00019: loss improved from 1.70467 to 1.68324, saving model to weights-improvement-19-1.6832.hdf5\n",
      "144243/144243 [==============================] - 536s - loss: 1.6832   \n",
      "Epoch 21/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.6587Epoch 00020: loss improved from 1.68324 to 1.65879, saving model to weights-improvement-20-1.6588.hdf5\n",
      "144243/144243 [==============================] - 527s - loss: 1.6588   \n",
      "Epoch 22/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.6421Epoch 00021: loss improved from 1.65879 to 1.64208, saving model to weights-improvement-21-1.6421.hdf5\n",
      "144243/144243 [==============================] - 532s - loss: 1.6421   \n",
      "Epoch 23/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.6196Epoch 00022: loss improved from 1.64208 to 1.61962, saving model to weights-improvement-22-1.6196.hdf5\n",
      "144243/144243 [==============================] - 537s - loss: 1.6196   \n",
      "Epoch 24/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.6011Epoch 00023: loss improved from 1.61962 to 1.60120, saving model to weights-improvement-23-1.6012.hdf5\n",
      "144243/144243 [==============================] - 530s - loss: 1.6012   \n",
      "Epoch 25/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.5844Epoch 00024: loss improved from 1.60120 to 1.58439, saving model to weights-improvement-24-1.5844.hdf5\n",
      "144243/144243 [==============================] - 532s - loss: 1.5844   \n",
      "Epoch 26/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.5694Epoch 00025: loss improved from 1.58439 to 1.56955, saving model to weights-improvement-25-1.5695.hdf5\n",
      "144243/144243 [==============================] - 528s - loss: 1.5695   \n",
      "Epoch 27/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.5534Epoch 00026: loss improved from 1.56955 to 1.55349, saving model to weights-improvement-26-1.5535.hdf5\n",
      "144243/144243 [==============================] - 535s - loss: 1.5535   \n",
      "Epoch 28/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.5353Epoch 00027: loss improved from 1.55349 to 1.53535, saving model to weights-improvement-27-1.5353.hdf5\n",
      "144243/144243 [==============================] - 535s - loss: 1.5353   \n",
      "Epoch 29/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.5211Epoch 00028: loss improved from 1.53535 to 1.52110, saving model to weights-improvement-28-1.5211.hdf5\n",
      "144243/144243 [==============================] - 536s - loss: 1.5211   \n",
      "Epoch 30/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.5073Epoch 00029: loss improved from 1.52110 to 1.50732, saving model to weights-improvement-29-1.5073.hdf5\n",
      "144243/144243 [==============================] - 531s - loss: 1.5073   \n",
      "Epoch 31/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.4990Epoch 00030: loss improved from 1.50732 to 1.49896, saving model to weights-improvement-30-1.4990.hdf5\n",
      "144243/144243 [==============================] - 532s - loss: 1.4990   \n",
      "Epoch 32/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.4836Epoch 00031: loss improved from 1.49896 to 1.48364, saving model to weights-improvement-31-1.4836.hdf5\n",
      "144243/144243 [==============================] - 532s - loss: 1.4836   \n",
      "Epoch 33/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.4726Epoch 00032: loss improved from 1.48364 to 1.47270, saving model to weights-improvement-32-1.4727.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144243/144243 [==============================] - 535s - loss: 1.4727   \n",
      "Epoch 34/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.4614Epoch 00033: loss improved from 1.47270 to 1.46161, saving model to weights-improvement-33-1.4616.hdf5\n",
      "144243/144243 [==============================] - 534s - loss: 1.4616   \n",
      "Epoch 35/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.4512Epoch 00034: loss improved from 1.46161 to 1.45113, saving model to weights-improvement-34-1.4511.hdf5\n",
      "144243/144243 [==============================] - 532s - loss: 1.4511   \n",
      "Epoch 36/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.4415Epoch 00035: loss improved from 1.45113 to 1.44133, saving model to weights-improvement-35-1.4413.hdf5\n",
      "144243/144243 [==============================] - 532s - loss: 1.4413   \n",
      "Epoch 37/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.4271Epoch 00036: loss improved from 1.44133 to 1.42717, saving model to weights-improvement-36-1.4272.hdf5\n",
      "144243/144243 [==============================] - 529s - loss: 1.4272   \n",
      "Epoch 38/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.4252Epoch 00037: loss improved from 1.42717 to 1.42507, saving model to weights-improvement-37-1.4251.hdf5\n",
      "144243/144243 [==============================] - 534s - loss: 1.4251   \n",
      "Epoch 39/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.4114Epoch 00038: loss improved from 1.42507 to 1.41134, saving model to weights-improvement-38-1.4113.hdf5\n",
      "144243/144243 [==============================] - 530s - loss: 1.4113   \n",
      "Epoch 40/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.4013Epoch 00039: loss improved from 1.41134 to 1.40148, saving model to weights-improvement-39-1.4015.hdf5\n",
      "144243/144243 [==============================] - 534s - loss: 1.4015   \n",
      "Epoch 41/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.3951Epoch 00040: loss improved from 1.40148 to 1.39507, saving model to weights-improvement-40-1.3951.hdf5\n",
      "144243/144243 [==============================] - 529s - loss: 1.3951   \n",
      "Epoch 42/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.3865Epoch 00041: loss improved from 1.39507 to 1.38664, saving model to weights-improvement-41-1.3866.hdf5\n",
      "144243/144243 [==============================] - 527s - loss: 1.3866   \n",
      "Epoch 43/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.3820Epoch 00042: loss improved from 1.38664 to 1.38199, saving model to weights-improvement-42-1.3820.hdf5\n",
      "144243/144243 [==============================] - 527s - loss: 1.3820   \n",
      "Epoch 44/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.3713Epoch 00043: loss improved from 1.38199 to 1.37112, saving model to weights-improvement-43-1.3711.hdf5\n",
      "144243/144243 [==============================] - 532s - loss: 1.3711   \n",
      "Epoch 45/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.3666Epoch 00044: loss improved from 1.37112 to 1.36680, saving model to weights-improvement-44-1.3668.hdf5\n",
      "144243/144243 [==============================] - 529s - loss: 1.3668   \n",
      "Epoch 46/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.3599Epoch 00045: loss improved from 1.36680 to 1.35990, saving model to weights-improvement-45-1.3599.hdf5\n",
      "144243/144243 [==============================] - 532s - loss: 1.3599   \n",
      "Epoch 47/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.3517Epoch 00046: loss improved from 1.35990 to 1.35167, saving model to weights-improvement-46-1.3517.hdf5\n",
      "144243/144243 [==============================] - 533s - loss: 1.3517   \n",
      "Epoch 48/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.3773Epoch 00047: loss did not improve\n",
      "144243/144243 [==============================] - 530s - loss: 1.3774   \n",
      "Epoch 49/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.3416Epoch 00048: loss improved from 1.35167 to 1.34148, saving model to weights-improvement-48-1.3415.hdf5\n",
      "144243/144243 [==============================] - 533s - loss: 1.3415   \n",
      "Epoch 50/50\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.3345Epoch 00049: loss improved from 1.34148 to 1.33437, saving model to weights-improvement-49-1.3344.hdf5\n",
      "144243/144243 [==============================] - 531s - loss: 1.3344   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f66ef18fe48>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit(X, y, epochs = 20, batch_size = 128, callbacks = callbacks_list)\n",
    "model.fit(X, y, epochs = 50, batch_size = 64, callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois de executar o fit, você deve ter uma série de arquivos de checkpoint no mesmo diretório onde está este Jupyter Notebook. Você pode excluí-los todos exceto aquele com o menor valor de perda. Por exemplo, neste caso, o arquivo weights-improvement-19-1.9119.hdf5 será usado. Ele contém os melhores valores de peso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Carrega os melhores pesos da rede e compila o modelo\n",
    "filename = \"weights-improvement-49-1.3344.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"  any sense, they'd take the\n",
      "roof off.' after a minute or two, they began moving about again, and\n",
      "ali \"\n",
      "ce was so soeeze the cook had to be a lett things the caby was the pueen sat again, and the doom was a little shale of the soom of the door with the soof of the door, and the soom aadk again, and she was a very curious senarking to herself 'it was a parhe mittle birds with the same with what it was a large canlessations and the soofs, and the soom as she was a very curious semark, and the doom had to say the caby with the soom of the door and the tooes, and she was not and sooetheng about the caby was the pueen said to her eyes, and the soom aadk again, and she was a very curious cropiens and the caby was snon and seemed to be a lettor of tee it was a great duactle the caby was the pueen sat again, and the soom of the door and the soo of her head to be tee with the same with with the sooes, and the soom of the door and the soo of her head to be a little book with the soom of the door, and the soom of the door and the thing was a good deal of she was to sook a little shale of the soofsh\n",
      "Concluído.\n"
     ]
    }
   ],
   "source": [
    "# Obtém um random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "\n",
    "# Inicia a geração de texto de um ponto qualquer, definido pelo random seed \"start\"\n",
    "pattern = dataX[start]\n",
    "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "\n",
    "# Gerando caracteres\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print (\"\\nConcluído.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Abaixo algumas sugestões para melhorar este modelo:\n",
    "\n",
    "* Prever menos de 1.000 caracteres como saída para uma determinada semente (seed).\n",
    "* Remova toda a pontuação do texto original e, portanto, do vocabulário do modelo.\n",
    "* Experimente One-Hot Encoding para as sequências de entrada.\n",
    "* Aumente o número de épocas de treinamento para 100 ou muitas centenas (isso pode levar até dias para o treinamento, mas aumentará a previsão do modelo.\n",
    "* Ajuste o percentual de Dropout\n",
    "* Adicione mais unidades de memória às camadas e/ou mais camadas."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
